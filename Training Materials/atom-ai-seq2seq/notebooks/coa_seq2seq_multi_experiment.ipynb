{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r project/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devquinn\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import statistics\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_types(file_list):\n",
    "    \n",
    "    file_extensions = []\n",
    "    for file in file_list:\n",
    "        extension = file.split('.')[-1]\n",
    "        file_extensions.append(extension)\n",
    "    \n",
    "    if len(set(file_extensions)) > 1:\n",
    "        raise TypeError('Please make sure all specified files are of the same type (csv or excel)')\n",
    "\n",
    "    file_extension = set(file_extensions).pop()\n",
    "\n",
    "    if(file_extension not in ['xlsx', 'csv']):\n",
    "        raise ValueError('Invalid file type. Acceptable files include: csv, excel')\n",
    "        \n",
    "    return file_extension\n",
    "\n",
    "\n",
    "def create_merged_df(data_path, file_list, header_list, sheet_list, file_type):\n",
    "    \n",
    "    if len(file_list) != len(header_list):\n",
    "        raise ValueError('Lengths of file_list and header_list do not match')\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    targets_check = []\n",
    "    \n",
    "    for f in range(len(file_list)):\n",
    "        \n",
    "        file_name = file_list[f]\n",
    "        header = header_list[f]\n",
    "        \n",
    "        if file_type == 'csv':\n",
    "            df_file = pd.read_csv(data_path+file_name, dtype=str, header=header)\n",
    "        elif file_type == 'xlsx':\n",
    "            sheet_name = sheet_list[f]\n",
    "            df_file = pd.read_excel(data_path+file_name, engine='openpyxl', dtype=str, header=header, sheet_name=sheet_name)\n",
    "        else: \n",
    "            raise ValueError('Invalid input for file_type. Acceptable inputs include: csv, excel')\n",
    "\n",
    "        if('Target Account Description' in df_file.columns): targets_check.append(1)\n",
    "        else: targets_check.append(0)\n",
    "            \n",
    "        df = pd.concat([df, df_file], sort=False)\n",
    "\n",
    "\n",
    "    if(len(set(targets_check)) > 1):\n",
    "        raise ValueError('Inconsistency with inclusion/exclusion of target data between specified files')\n",
    "\n",
    "    have_targets = set(targets_check).pop()\n",
    "        \n",
    "    return df, have_targets\n",
    "\n",
    "def format_df(df, model_type, have_targets):\n",
    "\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    if model_type == 'long':\n",
    "        if(any(['Source Level 1' not in df.columns, 'Source Level 2' not in df.columns, 'Source Level 3' not in df.columns])):\n",
    "            raise ValueError(\"Model type 'long' selected, but source levels with required name format could not be found in data frame columns\")\n",
    "        source_data = pd.DataFrame({'Source Level 1': df['Source Level 1'].copy(), 'Source Level 2': df['Source Level 2'].copy(), \n",
    "                                    'Source Level 3': df['Source Level 3'].copy(), 'Source Account Description': df['Source Account Description'].copy()})  \n",
    "        \n",
    "        if(have_targets):\n",
    "            target_data = pd.DataFrame({'Target Level 1': df['Target Level 1'].copy(), 'Target Level 2': df['Target Level 2'].copy(), \n",
    "                                        'Target Level 3': df['Target Level 3'].copy(), 'Target Account Description': df['Target Account Description'].copy()})\n",
    "        else: target_data = None\n",
    "        \n",
    "    elif model_type == 'short':\n",
    "        source_data = pd.DataFrame({'Source Account Description': df['Source Account Description'].copy()})\n",
    "        \n",
    "        if(have_targets):\n",
    "            target_data = pd.DataFrame({'Target Account Description': df['Target Account Description'].copy()})\n",
    "        else: target_data = None\n",
    "        \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "\n",
    "    df = pd.concat([source_data, target_data], axis=1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = str(text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('â€“', ' ', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    text = text.split(' ')\n",
    "   \n",
    "    words = []\n",
    "\n",
    "    for word in text:\n",
    "\n",
    "        #handle text with forward slash\n",
    "        fs_split = word.split('/')\n",
    "        if len(fs_split) > 1:\n",
    "            if(list(filter(lambda word: len(word) < 3, fs_split))): word = \"\".join(fs_split)\n",
    "            else: word = \" \".join(fs_split)\n",
    "\n",
    "        #handle text with ampersand \n",
    "        amp_split = word.split('&')\n",
    "        if len(amp_split) > 1:\n",
    "            if(list(filter(lambda word: len(word) < 3, amp_split))): word = \"\".join(amp_split)\n",
    "            else: word = \" \".join(amp_split)\n",
    "\n",
    "        #handle text with period abbreviations\n",
    "        per_split = word.split('.')\n",
    "        if len(per_split) > 1:\n",
    "            if(list(filter(lambda word: len(word) < 3, per_split))): word = \"\".join(per_split)\n",
    "            else: word = \" \".join(per_split)\n",
    "        \n",
    "        #handle single strings with words separated by capitals\n",
    "        chars = [char for char in word]\n",
    "\n",
    "        idx_slices = [0]\n",
    "        for i in range(len(chars)):\n",
    "            if i == 0: continue\n",
    "            #want to keep consecutive letters together\n",
    "            if chars[i-1] in string.ascii_uppercase: continue\n",
    "            if chars[i] in string.ascii_uppercase: idx_slices.append(i)\n",
    "\n",
    "        word = \" \".join([word[i:j] for i,j in zip(idx_slices, idx_slices[1:]+[None])])\n",
    "        words.append(word)\n",
    "    \n",
    "    text = \" \".join(words).lower().strip()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation+string.digits])\n",
    "    text = re.sub('\\\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    for col in df.columns:df[col] = df[col].apply(preprocess_text)\n",
    "    return df\n",
    "    \n",
    "def prepare_data_for_model(df, model_type, have_targets, test_size=0.2, shuffle=True):\n",
    "    \n",
    "    if(model_type == 'long'):\n",
    "        source_data = df.loc[:, 'Source Level 1':'Source Account Description']\n",
    "        if(have_targets): target_data = df.loc[:, 'Target Level 1':'Target Account Description']\n",
    "    else:\n",
    "        source_data = df['Source Account Description']\n",
    "        if(have_targets): target_data = df['Target Account Description']\n",
    "\n",
    "    if(have_targets):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(source_data, target_data, test_size=test_size, shuffle=shuffle)\n",
    "    else:\n",
    "        x_train, x_test = train_test_split(source_data, test_size=test_size, shuffle=shuffle)\n",
    "        y_train, y_test = None, None\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "\n",
    "def data_preprocessing_pipeline(data_path, file_list, header_list, sheet_list, model_type, test_size=0.2, verbose=True):\n",
    "\n",
    "    if(verbose): print('starting data preprocessing...')\n",
    "\n",
    "    if(verbose): print('checking given file(s) type... ', end=\" \", flush=True)\n",
    "    file_type = check_file_types(file_list)\n",
    "    if(verbose): print(f'{file_type} file detected')\n",
    "\n",
    "    if(verbose): print('merging files into single data frame... ', end=\" \", flush=True)\n",
    "    df, have_targets = create_merged_df(data_path, file_list, header_list, sheet_list, file_type)\n",
    "\n",
    "    if(verbose): print('formatting data frame... ', end=\" \", flush=True)\n",
    "    df = format_df(df, model_type, have_targets)\n",
    "\n",
    "    if(verbose): print('preprocessing data... ', end=\" \", flush=True)\n",
    "    df = preprocess_df(df)\n",
    "    if(verbose): print(f'done')\n",
    "\n",
    "    if(verbose): print(f'data frame created with {len(df)} entries')\n",
    "\n",
    "    x_train, x_test, y_train, y_test = prepare_data_for_model(df, model_type, have_targets, test_size=test_size, shuffle=True)\n",
    "\n",
    "    if(verbose): print(f'training size: {len(x_train)} \\t testing size: {len(x_test)}')\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "564                              employee meetings events\n",
       "1070    short term debt accounts receivable securitiza...\n",
       "701                                cost of goods sold fob\n",
       "261                                   imputed interest ar\n",
       "672                                contribution political\n",
       "                              ...                        \n",
       "1315                              cash pnc cad wdch us lp\n",
       "1470               other current liabilities md sales tax\n",
       "629                            gain loss on disp of fixed\n",
       "317                     withholding taxes payable foreign\n",
       "1457                            cash pnc usd chemtrade us\n",
       "Name: Source Account Description, Length: 1632, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:/Users/devquinn/OneDrive - Deloitte (O365D)/Desktop/grouped_data/experiment/\" #'/home/ec2-user/SageMaker/project/data/grouped_data/system/'\n",
    "\n",
    "file_list = ['Oracle.csv']\n",
    "header_list = [0]\n",
    "sheet_list = None\n",
    "model_type = 'short'\n",
    "test_size = 0.15\n",
    "\n",
    "x_train, x_test, y_train, y_test = data_preprocessing_pipeline(data_path, file_list, header_list, sheet_list, model_type, test_size=test_size, verbose=False)\n",
    "#experiment file has no targets\n",
    "print(y_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Division</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Source Account #</th>\n",
       "      <th>Source Account Description</th>\n",
       "      <th>Target Account #</th>\n",
       "      <th>Target Account Description</th>\n",
       "      <th>Source Level 1</th>\n",
       "      <th>Source Level 2</th>\n",
       "      <th>Source Level 3</th>\n",
       "      <th>...</th>\n",
       "      <th>Source Level 5</th>\n",
       "      <th>Source Level 6</th>\n",
       "      <th>Source Level 7</th>\n",
       "      <th>Source Level 8</th>\n",
       "      <th>Source Level 9</th>\n",
       "      <th>Source Level 10</th>\n",
       "      <th>Target Level 1</th>\n",
       "      <th>Target Level 2</th>\n",
       "      <th>Target Level 3</th>\n",
       "      <th>Target Level 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>101397</td>\n",
       "      <td>INV REVAL - GROSS SC</td>\n",
       "      <td>12250000</td>\n",
       "      <td>Inventory Reval. - gross - Std. costs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>100447</td>\n",
       "      <td>BAR07-SCOTIA-BDS</td>\n",
       "      <td>10000000</td>\n",
       "      <td>Cash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>101000</td>\n",
       "      <td>kookmin Bank(SeoCho)</td>\n",
       "      <td>10000000</td>\n",
       "      <td>Cash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>120305</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>15001300</td>\n",
       "      <td>PPE - Computer Hardware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAP</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>292000</td>\n",
       "      <td>Paid in Capital</td>\n",
       "      <td>29030000</td>\n",
       "      <td>Paid in Capital</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>MSDynamics</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Misc Apparel and Accessories</td>\n",
       "      <td>02-kbpri002</td>\n",
       "      <td>Price Grabber</td>\n",
       "      <td>200101</td>\n",
       "      <td>Accounts Payable - Trade</td>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Current Liabilities</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Current Liabilities</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>MSDynamics</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Misc Apparel and Accessories</td>\n",
       "      <td>02-07955</td>\n",
       "      <td>Usizy Labs S.L</td>\n",
       "      <td>299999</td>\n",
       "      <td>Liabilities - History</td>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Current Liabilities</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liabilities - History</td>\n",
       "      <td>Liabilities - History</td>\n",
       "      <td>Liabilities - History</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>MSDynamics</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Misc Apparel and Accessories</td>\n",
       "      <td>102110</td>\n",
       "      <td>Ar - Pos (Retail Stores)</td>\n",
       "      <td>101001</td>\n",
       "      <td>Accounts Receivable - Trade</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Current Assets</td>\n",
       "      <td>Accounts Receivable</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Current Assets</td>\n",
       "      <td>Accounts Receivable</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>MSDynamics</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Misc Apparel and Accessories</td>\n",
       "      <td>02-kblei005</td>\n",
       "      <td>Leicester City Council</td>\n",
       "      <td>200101</td>\n",
       "      <td>Accounts Payable - Trade</td>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Current Liabilities</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Current Liabilities</td>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>MSDynamics</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Misc Apparel and Accessories</td>\n",
       "      <td>101205</td>\n",
       "      <td>Pnc-nutmeg</td>\n",
       "      <td>100175</td>\n",
       "      <td>PNC - Nutmeg</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Current Assets</td>\n",
       "      <td>Cash and Cash Equivalents</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Current Assets</td>\n",
       "      <td>Cash and Cash Equivalents</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22527 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          System       Division                      Industry  \\\n",
       "0            SAP  Manufacturing                         Drugs   \n",
       "1            SAP  Manufacturing                         Drugs   \n",
       "2            SAP  Manufacturing                         Drugs   \n",
       "3            SAP  Manufacturing                         Drugs   \n",
       "4            SAP  Manufacturing                         Drugs   \n",
       "...          ...            ...                           ...   \n",
       "1370  MSDynamics  Manufacturing  Misc Apparel and Accessories   \n",
       "1371  MSDynamics  Manufacturing  Misc Apparel and Accessories   \n",
       "1372  MSDynamics  Manufacturing  Misc Apparel and Accessories   \n",
       "1373  MSDynamics  Manufacturing  Misc Apparel and Accessories   \n",
       "1374  MSDynamics  Manufacturing  Misc Apparel and Accessories   \n",
       "\n",
       "     Source Account # Source Account Description Target Account #  \\\n",
       "0              101397       INV REVAL - GROSS SC         12250000   \n",
       "1              100447           BAR07-SCOTIA-BDS         10000000   \n",
       "2              101000       kookmin Bank(SeoCho)         10000000   \n",
       "3              120305          Computer Hardware         15001300   \n",
       "4              292000            Paid in Capital         29030000   \n",
       "...               ...                        ...              ...   \n",
       "1370      02-kbpri002              Price Grabber           200101   \n",
       "1371         02-07955             Usizy Labs S.L           299999   \n",
       "1372           102110   Ar - Pos (Retail Stores)           101001   \n",
       "1373      02-kblei005     Leicester City Council           200101   \n",
       "1374           101205                 Pnc-nutmeg           100175   \n",
       "\n",
       "                 Target Account Description Source Level 1  \\\n",
       "0     Inventory Reval. - gross - Std. costs            NaN   \n",
       "1                                      Cash            NaN   \n",
       "2                                      Cash            NaN   \n",
       "3                   PPE - Computer Hardware            NaN   \n",
       "4                           Paid in Capital            NaN   \n",
       "...                                     ...            ...   \n",
       "1370               Accounts Payable - Trade    Liabilities   \n",
       "1371                  Liabilities - History    Liabilities   \n",
       "1372            Accounts Receivable - Trade         Assets   \n",
       "1373               Accounts Payable - Trade    Liabilities   \n",
       "1374                           PNC - Nutmeg         Assets   \n",
       "\n",
       "           Source Level 2             Source Level 3  ... Source Level 5  \\\n",
       "0                     NaN                        NaN  ...            NaN   \n",
       "1                     NaN                        NaN  ...            NaN   \n",
       "2                     NaN                        NaN  ...            NaN   \n",
       "3                     NaN                        NaN  ...            NaN   \n",
       "4                     NaN                        NaN  ...            NaN   \n",
       "...                   ...                        ...  ...            ...   \n",
       "1370  Current Liabilities           Accounts Payable  ...            NaN   \n",
       "1371  Current Liabilities           Accounts Payable  ...            NaN   \n",
       "1372       Current Assets        Accounts Receivable  ...            NaN   \n",
       "1373  Current Liabilities           Accounts Payable  ...            NaN   \n",
       "1374       Current Assets  Cash and Cash Equivalents  ...            NaN   \n",
       "\n",
       "     Source Level 6 Source Level 7 Source Level 8 Source Level 9  \\\n",
       "0               NaN            NaN            NaN            NaN   \n",
       "1               NaN            NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN            NaN   \n",
       "...             ...            ...            ...            ...   \n",
       "1370            NaN            NaN            NaN            NaN   \n",
       "1371            NaN            NaN            NaN            NaN   \n",
       "1372            NaN            NaN            NaN            NaN   \n",
       "1373            NaN            NaN            NaN            NaN   \n",
       "1374            NaN            NaN            NaN            NaN   \n",
       "\n",
       "     Source Level 10         Target Level 1         Target Level 2  \\\n",
       "0                NaN                    NaN                    NaN   \n",
       "1                NaN                    NaN                    NaN   \n",
       "2                NaN                    NaN                    NaN   \n",
       "3                NaN                    NaN                    NaN   \n",
       "4                NaN                    NaN                    NaN   \n",
       "...              ...                    ...                    ...   \n",
       "1370             NaN            Liabilities    Current Liabilities   \n",
       "1371             NaN  Liabilities - History  Liabilities - History   \n",
       "1372             NaN                 Assets         Current Assets   \n",
       "1373             NaN            Liabilities    Current Liabilities   \n",
       "1374             NaN                 Assets         Current Assets   \n",
       "\n",
       "                 Target Level 3 Target Level 4  \n",
       "0                           NaN            NaN  \n",
       "1                           NaN            NaN  \n",
       "2                           NaN            NaN  \n",
       "3                           NaN            NaN  \n",
       "4                           NaN            NaN  \n",
       "...                         ...            ...  \n",
       "1370           Accounts Payable            NaN  \n",
       "1371      Liabilities - History            NaN  \n",
       "1372        Accounts Receivable            NaN  \n",
       "1373           Accounts Payable            NaN  \n",
       "1374  Cash and Cash Equivalents            NaN  \n",
       "\n",
       "[22527 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:/Users/devquinn/OneDrive - Deloitte (O365D)/Desktop/grouped_data/system/\" #'/home/ec2-user/SageMaker/project/data/grouped_data/system/'\n",
    "\n",
    "file_list = ['SAP.csv', 'Oracle.csv', 'MSDynamics.csv']\n",
    "header_list = [0, 0 , 0]\n",
    "sheet_list = None\n",
    "\n",
    "#look at original data\n",
    "df, _ = create_merged_df(data_path, file_list, header_list, sheet_list, file_type=check_file_types(file_list))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting data preprocessing...\n",
      "checking given file(s) type...  csv file detected\n",
      "merging files into single data frame...  formatting data frame...  preprocessing data...  done\n",
      "data frame created with 22527 entries\n",
      "training size: 19147 \t testing size: 3380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Level 1</th>\n",
       "      <th>Source Level 2</th>\n",
       "      <th>Source Level 3</th>\n",
       "      <th>Source Account Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>m mars financial statement v full trial balan</td>\n",
       "      <td>net financing</td>\n",
       "      <td>total treasury assets</td>\n",
       "      <td>bank account transfers in receipts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>accum amort outlicensed tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>ic wme allocation staff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Source Level 1 Source Level 2  \\\n",
       "13312  m mars financial statement v full trial balan  net financing   \n",
       "1382                                             nan            nan   \n",
       "389                                              nan            nan   \n",
       "\n",
       "              Source Level 3          Source Account Description  \n",
       "13312  total treasury assets  bank account transfers in receipts  \n",
       "1382                     nan        accum amort outlicensed tech  \n",
       "389                      nan             ic wme allocation staff  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'long'\n",
    "test_size = 0.15\n",
    "\n",
    "x_train, x_test, y_train, y_test = data_preprocessing_pipeline(data_path, file_list, header_list, sheet_list, model_type, test_size=test_size, verbose=True)\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Level 1</th>\n",
       "      <th>Target Level 2</th>\n",
       "      <th>Target Level 3</th>\n",
       "      <th>Target Account Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>assets</td>\n",
       "      <td>cash investments</td>\n",
       "      <td>cash</td>\n",
       "      <td>bank account transfers in receipts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>intan assets accum amort outlicensed tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>lt ar ic twx intraco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target Level 1    Target Level 2 Target Level 3  \\\n",
       "13312         assets  cash investments           cash   \n",
       "1382             nan               nan            nan   \n",
       "389              nan               nan            nan   \n",
       "\n",
       "                      Target Account Description  \n",
       "13312         bank account transfers in receipts  \n",
       "1382   intan assets accum amort outlicensed tech  \n",
       "389                         lt ar ic twx intraco  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model_name, losses, n_iters, benchmark_every, learning_rate):\n",
    "    x = np.arange(0, n_iters, benchmark_every)\n",
    "    y = losses\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.title('Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    #current_dir = os.getcwd()\n",
    "    # save_to = os.path.join(current_dir, r'loss_plots')\n",
    "    # if not os.path.exists(save_to): os.makedirs(save_to)\n",
    "    # plt.savefig(os.path.join(save_to, f'loss_{model_name}_{learning_rate}.jpg'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNKNOWN\"}\n",
    "        self.n_words = 3  # Count SOS, EOS, UNKNOWN\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(x_data, y_data):\n",
    "\n",
    "    x_data = x_data.reset_index(drop=True)\n",
    "    y_data = y_data.reset_index(drop=True)\n",
    "\n",
    "    sources, targets = [], []\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        source_level_text, target_level_text = [], []\n",
    "\n",
    "        for col in x_data:\n",
    "            s_text = x_data.loc[i, col]\n",
    "            source_level_text.append(s_text)\n",
    "        for col in y_data:\n",
    "            t_text = y_data.loc[i, col]\n",
    "            target_level_text.append(t_text)\n",
    "\n",
    "        sources.append(source_level_text)\n",
    "        targets.append(target_level_text)\n",
    "\n",
    "    pairs = list((zip(sources, targets)))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def create_vocabulary(set1, set2, x_data, y_data):\n",
    "    \n",
    "    input_set = Vocabulary(set1)\n",
    "    output_set = Vocabulary(set2)\n",
    "    pairs = create_pairs(x_data, y_data)\n",
    "\n",
    "    for pair in pairs:\n",
    "        full_source_string = \" \".join(pair[0])\n",
    "        full_target_string = \" \".join(pair[1])\n",
    "        input_set.add_sentence(full_source_string)\n",
    "        output_set.add_sentence(full_target_string)\n",
    "\n",
    "    return input_set, output_set, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(set, sentence):\n",
    "    sentence_indexes = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word not in set.word2index:\n",
    "            sentence_indexes.append(2)\n",
    "        else:\n",
    "            sentence_indexes.append(set.word2index[word])\n",
    "            \n",
    "    return sentence_indexes\n",
    "\n",
    "def tensor_from_sentence(set, sentence):\n",
    "    indexes = indexes_from_sentence(set, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensors_from_text_list(text_list, input_set, output_set):\n",
    "    tensors = []\n",
    "    for level in text_list:\n",
    "        print(level)\n",
    "        tensor = tensor_from_sentence(input_set, level)\n",
    "        tensors.append(tensor)\n",
    "        \n",
    "    return tensors\n",
    "\n",
    "def tensors_from_pair(pair, input_set, output_set):\n",
    "    source = pair[0]\n",
    "    target = pair[1]\n",
    "    source_target_pairwise_tensors = []\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        source_level = source[i]\n",
    "        target_level = target[i]\n",
    "        source_level_tensor = tensor_from_sentence(input_set, source_level)\n",
    "        target_level_tensor = tensor_from_sentence(output_set, target_level)\n",
    "        source_target_pairwise_tensors.append((source_level_tensor, target_level_tensor))\n",
    "\n",
    "    return source_target_pairwise_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.name = 'Decoder'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.name = 'AttnDecoder'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(tensors_pair, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, model_type, max_length=100, teacher_forcing_ratio=0.0):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    if(model_type == 'long'):\n",
    "        L1_loss, L2_loss, L3_loss, L4_loss, report_loss = 0, 0, 0, 0, 0\n",
    "        level_losses = [L1_loss, L2_loss, L3_loss, L4_loss]\n",
    "        \n",
    "    elif(model_type=='short'):\n",
    "        description_loss, report_loss = 0, 0\n",
    "        level_losses = [description_loss]\n",
    "        \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "          \n",
    "    target_lengths = []\n",
    "\n",
    "    for i in range(len(tensors_pair)):\n",
    "        source_level_tensor = tensors_pair[i][0]\n",
    "        target_level_tensor = tensors_pair[i][1]\n",
    "        source_level_length = source_level_tensor.size(0)\n",
    "        target_level_length = target_level_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        #loop through each word of the source level\n",
    "        for en in range(source_level_length):\n",
    "            encoder_output, encoder_hidden = encoder(source_level_tensor[en], encoder_hidden)\n",
    "            encoder_outputs[en] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for de in range(target_level_length):\n",
    "                if(decoder.name == 'AttnDecoder'):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                \n",
    "                level_losses[i] += criterion(decoder_output, target_level_tensor[de])\n",
    "                decoder_input = target_level_tensor[de]  # Teacher forcing\n",
    "\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for de in range(target_level_length):\n",
    "                if(decoder.name == 'AttnDecoder'):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                level_losses[i] += criterion(decoder_output, target_level_tensor[de])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "            \n",
    "        level_losses[i].backward(retain_graph=True)\n",
    "        target_lengths.append(target_level_length)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    report_loss = level_losses[-1].item()\n",
    "\n",
    "    return report_loss / target_lengths[-1]\n",
    "\n",
    "\n",
    "def train_model(input_set, output_set, encoder, decoder, training_pairs, validation_pairs, n_iters, save_path, model_name, model_type, hidden_size, dropout=None, attention=False, benchmark_every=100, \n",
    "                save_every=100, learning_rate=0.01, tf_ratio=0.0, max_length=100, verbose=False):\n",
    "    \n",
    "    if (save_every > n_iters) or (benchmark_every > n_iters):\n",
    "        raise ValueError('Parameter num_iters must be larger than paramters save_every and benchmark_loss_every')\n",
    "    if n_iters % benchmark_every != 0:\n",
    "        raise ValueError('Parameter num_iters must be evenly divisible by parameter benchmark_loss_every')\n",
    "\n",
    "    if(verbose): print('\\ntraining model...')\n",
    "    loss_total = 0  # Reset every benchmark_every\n",
    "    losses = []\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #create training pairs randomly from given list of pairs\n",
    "    training_pairs = [tensors_from_pair(random.choice(training_pairs), input_set, output_set) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    val_scores = []\n",
    "    #run iteration for each training pair\n",
    "    for i in tqdm(range(1, n_iters + 1)):\n",
    "        tensors_pair = training_pairs[i - 1]\n",
    "\n",
    "        loss = run_iter(tensors_pair, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "                        criterion, model_type, max_length=max_length, teacher_forcing_ratio=tf_ratio)\n",
    "        \n",
    "        loss_total += loss\n",
    "        #record loss\n",
    "        if i % benchmark_every == 0:\n",
    "            loss_avg = loss_total / benchmark_every\n",
    "            losses.append(loss_avg)\n",
    "            loss_total = 0\n",
    "\n",
    "            val_score = validate(validation_pairs, input_set, output_set, encoder, decoder, max_length, model_type)\n",
    "            val_scores.append(val_score)\n",
    "\n",
    "            if len(val_scores) > 4:\n",
    "                current_score = val_scores[-1]\n",
    "                avg_previous_scores = np.mean(val_scores[-5:-1])\n",
    "                print(current_score, avg_previous_scores)\n",
    "\n",
    "        #save model parameters\n",
    "        if i % save_every == 0:\n",
    "            torch.save({'en_sd': encoder.state_dict(),\n",
    "                        'de_sd': decoder.state_dict(),\n",
    "                        'en_opt': encoder_optimizer,\n",
    "                        'de_opt': decoder_optimizer,\n",
    "                        'en_opt_sd': encoder_optimizer.state_dict(),\n",
    "                        'de_opt_sd': decoder_optimizer.state_dict(),\n",
    "                        'loss': losses[-1],\n",
    "                        'input_dict': input_set.__dict__,\n",
    "                        'output_dict': output_set.__dict__,\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'dropout': dropout, \n",
    "                        'max_length': max_length,\n",
    "                        'attention': attention,\n",
    "                        'model_type': model_type,\n",
    "                        }, os.path.join(save_path, '{}_{}_{:.3f}.hdf5'.format(model_name, i, losses[-1])))\n",
    "\n",
    "    plot_loss(model_name, losses, n_iters, benchmark_every, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_pairs, input_set, output_set, encoder, decoder, max_length, model_type):\n",
    "    \n",
    "    predictions, targets = [], []\n",
    "    for pair in validation_pairs:\n",
    "        source = pair[0]\n",
    "        target = pair[1]\n",
    "                \n",
    "        output, _, _ = predict(input_set, output_set, encoder, decoder, source, max_length=max_length)\n",
    "        prediction = []\n",
    "        for pred_level in output:\n",
    "            pred_level = \" \".join(pred_level[:-1]) #drop EOS token\n",
    "            prediction.append(pred_level)\n",
    "                \n",
    "        predictions.append(prediction) \n",
    "        targets.append(target)\n",
    "\n",
    "    _, _, _, l4_scores, _ = calc_scores(predictions, targets, model_type=model_type)\n",
    "\n",
    "    val_bleu_score = np.mean(l4_scores)\n",
    "\n",
    "    return val_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 2891/100000 [15:57<8:56:10,  3.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-1dce3d45eb8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m train_model(input_set, output_set, encoder, decoder, pairs_train, pairs_val, n_iters=100000, save_path=save_path, model_name='long_model',\n\u001b[0m\u001b[0;32m     10\u001b[0m             model_type=model_type, hidden_size=256, dropout=0.1, attention=True, save_every=2000, learning_rate=0.01, benchmark_every=1000, verbose=True)\n",
      "\u001b[1;32m<ipython-input-117-4839a80cdb48>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(input_set, output_set, encoder, decoder, training_pairs, validation_pairs, n_iters, save_path, model_name, model_type, hidden_size, dropout, attention, benchmark_every, save_every, learning_rate, tf_ratio, max_length, verbose)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mtensors_pair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         loss = run_iter(tensors_pair, encoder, decoder, encoder_optimizer, decoder_optimizer, \n\u001b[0m\u001b[0;32m     99\u001b[0m                         criterion, model_type, max_length=max_length, teacher_forcing_ratio=tf_ratio)\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-4839a80cdb48>\u001b[0m in \u001b[0;36mrun_iter\u001b[1;34m(tensors_pair, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, model_type, max_length, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mlevel_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_level_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_set, output_set, pairs = create_vocabulary('legacy', 'new', x_train, y_train)\n",
    "pairs_train, pairs_val = train_test_split(pairs, test_size=0.15, shuffle=True)\n",
    "\n",
    "encoder = Encoder(input_set.n_words, 256).to(device)\n",
    "decoder = AttnDecoder(256, output_set.n_words).to(device)\n",
    "\n",
    "save_path = os.getcwd()\n",
    "\n",
    "train_model(input_set, output_set, encoder, decoder, pairs_train, pairs_val, n_iters=100000, save_path=save_path, model_name='long_model',\n",
    "            model_type=model_type, hidden_size=256, dropout=0.1, attention=True, save_every=2000, learning_rate=0.01, benchmark_every=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_set, output_set, encoder, decoder, account, max_length=100):\n",
    "\n",
    "    level_confidence_scores = []\n",
    "    decoded_output = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(len(account)):\n",
    "            input_level = account[i]\n",
    "            input_level_tensor = tensor_from_sentence(input_set, input_level)\n",
    "            input_level_length = input_level_tensor.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "            for en in range(input_level_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_level_tensor[en], encoder_hidden)\n",
    "                encoder_outputs[en] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_level_words = []\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "            #record values for each decoded word\n",
    "            top_values = []\n",
    "\n",
    "            for de in range(max_length):\n",
    "                if(decoder.name == 'AttnDecoder'):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    decoder_attentions[de] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                top_values.append(np.exp(topv.item())*100)\n",
    "\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_level_words.append('<EOS>')\n",
    "                    break\n",
    "                else:\n",
    "                    #translate decoder output into word and append\n",
    "                    decoded_level_words.append(output_set.index2word[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            decoded_output.append(decoded_level_words)\n",
    "            level_confidence = statistics.mean(top_values)\n",
    "            level_confidence_scores.append(level_confidence)\n",
    "            \n",
    "        #take average of decoded word values as confidence score\n",
    "        confidence_score = statistics.mean(level_confidence_scores)\n",
    "\n",
    "        return decoded_output, decoder_attentions[:de + 1], confidence_score\n",
    "    \n",
    "            \n",
    "def predict_on_new(input_set, output_set, encoder, decoder, model_type, df, max_length=100, verbose=False):\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "        source_data = {'Source Account Description': df['Source Account Description'].copy()}\n",
    "        \n",
    "    elif(model_type == 'long'):     \n",
    "        source_data = {'Source Level 1': df['Source Level 1'].copy(),\n",
    "                       'Source Level 2': df['Source Level 2'].copy(),\n",
    "                       'Source Level 3': df['Source Level 3'].copy(),\n",
    "                       'Source Account Description': df['Source Account Description'].copy()}\n",
    "        \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "\n",
    "    sources = pd.DataFrame(source_data)\n",
    "    \n",
    "    if(verbose): print('Generating predictions...')\n",
    "    predictions, confidence_scores = [], []\n",
    "    for a in tqdm(range(len(sources))):\n",
    "        account = sources.iloc[a, :].tolist()\n",
    "        #predict on input account\n",
    "        output, _, confidence_score = predict(input_set, output_set, encoder, decoder, account, max_length=max_length)\n",
    "        prediction = []\n",
    "        for pred_level in output:\n",
    "            pred_level = \" \".join(pred_level[:-1]) #drop EOS token\n",
    "            prediction.append(pred_level)\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        confidence_scores.append(confidence_score)\n",
    "\n",
    "    df_out = sources.copy()\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "            df_out['Prediction Account Description'] = [p[0] for p in predictions] #remove list brackets from each acct\n",
    "            df_out['Confidence Score'] = confidence_scores\n",
    "    else:     \n",
    "        df_out['Prediction Level 1'] = list(list(zip(*predictions))[0])\n",
    "        df_out['Prediction Level 2'] = list(list(zip(*predictions))[1])\n",
    "        df_out['Prediction Level 3'] = list(list(zip(*predictions))[2])\n",
    "        df_out['Prediction Account Description'] = list(list(zip(*predictions))[3])\n",
    "        df_out['Confidence Score'] = confidence_scores\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def evaluate_output(df_predictions, df_with_targets, model_type, verbose=True):\n",
    "    \n",
    "    #df_with_targets = df_with_targets.reset_index(drop=True)\n",
    "    if(model_type == 'short'):\n",
    "        target_data = {'Target Account Description': df_with_targets['Target Account Description']}\n",
    "        \n",
    "    elif(model_type == 'long'):\n",
    "        target_data = {'Target Level 1': df_with_targets['Target Level 1'],\n",
    "                       'Target Level 2': df_with_targets['Target Level 2'],\n",
    "                       'Target Level 3': df_with_targets['Target Level 3'],\n",
    "                       'Target Account Description': df_with_targets['Target Account Description']}\n",
    "    \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "    \n",
    "    df_targets = pd.DataFrame(target_data)\n",
    "    \n",
    "    predictions, targets = [], []\n",
    "    for i in range(len(df_predictions)):\n",
    "        if(model_type == 'short'):\n",
    "            predictions.append(df_predictions.loc[i, 'Prediction Account Description'])\n",
    "        else:\n",
    "            predictions.append(df_predictions.loc[i, 'Prediction Level 1':'Prediction Account Description'].tolist())\n",
    "        targets.append(df_targets.iloc[i, :].tolist())\n",
    "    \n",
    "    l1_scores, l2_scores, l3_scores, l4_scores, description_similarities = calc_scores(predictions, targets, model_type)\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "        score_data = {'Predicted Description Bleu Score': l4_scores,\n",
    "                      'Predicted Description Similarity':description_similarities}\n",
    "        if(verbose):\n",
    "             print(f'\\nDescription Bleu score average: {np.mean(l4_scores):.2f} \\\n",
    "                     \\nAverage predicted description (L4) similarity to target: {np.mean(description_similarities)*100:.2f}%')\n",
    "    else:\n",
    "        score_data = {'Predicted Level 1 Bleu Score': l1_scores, \n",
    "                      'Predicted Level 2 Bleu Score': l2_scores,\n",
    "                      'Predicted Level 3 Bleu Score': l3_scores,\n",
    "                      'Predicted Description Bleu Score': l4_scores,\n",
    "                      'Predicted Description Similarity':description_similarities}\n",
    "        if(verbose):\n",
    "             print(f'\\nL1 Bleu score average: {np.mean(l1_scores):.2f} \\\n",
    "                     \\nL2 Bleu score average: {np.mean(l2_scores):.2f} \\\n",
    "                     \\nL3 Bleu score average: {np.mean(l3_scores):.2f} \\\n",
    "                     \\nL4 Bleu score average: {np.mean(l4_scores):.2f} \\\n",
    "                     \\nAverage predicted description (L4) similarity to target: {np.mean(description_similarities)*100:.2f}%')\n",
    "    \n",
    "    df_scores = pd.DataFrame(score_data)\n",
    "    \n",
    "    df_out = pd.concat([df_predictions, df_targets, df_scores], axis=1)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['m mars financial statement v full trial balan',\n",
       "  'net financing',\n",
       "  'total treasury assets',\n",
       "  'bank account transfers in receipts'],\n",
       " ['assets', 'cash investments', 'cash', 'bank account transfers in receipts'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_set, output_set, pairs_train = create_vocabulary('legacy', 'new', x_train, y_train)\n",
    "_, _, pairs_test = create_vocabulary('legacy_test', 'new_test', x_test, y_test)\n",
    "pairs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_set.n_words, 256).to(device)\n",
    "decoder = AttnDecoder(256, output_set.n_words).to(device)\n",
    "\n",
    "save_path = os.getcwd()\n",
    "\n",
    "train_model(input_set, output_set, encoder, decoder, pairs, n_iters=100000, save_path=save_path, model_name='long_model',\n",
    "            model_type='long', hidden_size=256, dropout=0.1, attention=True, save_every=2000, learning_rate=0.01, benchmark_every=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_score(reference, hypothesis):\n",
    "    reference = [reference.split()]\n",
    "    hypothesis = hypothesis.split()\n",
    "    return round(sentence_bleu(reference, hypothesis, weights=(1,)), 3)\n",
    "    \n",
    "    \n",
    "def calc_scores(predictions, targets, model_type):\n",
    "\n",
    "    l1_scores, l2_scores, l3_scores, l4_scores = [],[],[],[]\n",
    "    description_similarities = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        tar = targets[i]\n",
    "        \n",
    "        if(model_type == 'short'):\n",
    "            bleu_score = calc_bleu_score(tar[0], pred)\n",
    "            description_similarity = fuzz.token_sort_ratio(tar, pred)/100\n",
    "            description_similarities.append(description_similarity)\n",
    "            l4_scores.append(bleu_score)\n",
    "            \n",
    "        elif(model_type == 'long'):        \n",
    "            for j in range(len(pred)):\n",
    "                bleu_score = calc_bleu_score(tar[j], pred[j])\n",
    "                if j == 0: l1_scores.append(bleu_score)\n",
    "                elif j == 1: l2_scores.append(bleu_score)\n",
    "                elif j == 2: l3_scores.append(bleu_score)\n",
    "                else: \n",
    "                    l4_scores.append(bleu_score)\n",
    "                    description_similarity = fuzz.token_sort_ratio(tar[j], pred[j])/100\n",
    "                    description_similarities.append(description_similarity)\n",
    "                    break\n",
    "                    \n",
    "        else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "            \n",
    "\n",
    "    return l1_scores, l2_scores, l3_scores, l4_scores, description_similarities\n",
    "\n",
    "def load_model(model_path, model_name, device, attention=False):\n",
    "\n",
    "    model = torch.load(model_path+model_name, map_location=device)\n",
    "    model_type = model['model_type']\n",
    "\n",
    "    max_length = model['max_length']\n",
    "    input_set = Vocabulary('legacy')\n",
    "    input_set.__dict__ = model['input_dict']\n",
    "    output_set = Vocabulary('new')\n",
    "    output_set.__dict__ = model['output_dict']\n",
    "\n",
    "    encoder = Encoder(input_set.n_words, model['hidden_size']).to(device)\n",
    "\n",
    "    attention = model['attention']\n",
    "    if attention == True: \n",
    "        decoder = AttnDecoder(model['hidden_size'], output_set.n_words, dropout_p=0.1, max_length=max_length).to(device)\n",
    "    else:\n",
    "        decoder = Decoder(model['hidden_size'], output_set.n_words).to(device)\n",
    "\n",
    "    encoder.load_state_dict(model['en_sd'])\n",
    "    decoder.load_state_dict(model['de_sd'])\n",
    "\n",
    "    encoder_optimizer = model['en_opt']\n",
    "    decoder_optimizer = model['de_opt']\n",
    "    encoder_optimizer.load_state_dict(model['en_opt_sd'])\n",
    "    decoder_optimizer.load_state_dict(model['de_opt_sd'])\n",
    "\n",
    "    return input_set, output_set, encoder, decoder, max_length, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_set, output_set, encoder, decoder, account, model_type, max_length=100):\n",
    "\n",
    "    level_confidence_scores = []\n",
    "    decoded_output = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(len(account)):\n",
    "            input_level = account[i]\n",
    "            input_level_tensor = tensor_from_sentence(input_set, input_level)\n",
    "            input_level_length = input_level_tensor.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "            for en in range(input_level_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_level_tensor[en], encoder_hidden)\n",
    "                encoder_outputs[en] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_level_words = []\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "            #record values for each decoded word\n",
    "            top_values = []\n",
    "\n",
    "            for de in range(max_length):\n",
    "                if(decoder.name == 'AttnDecoder'):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    decoder_attentions[de] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                top_values.append(np.exp(topv.item())*100)\n",
    "\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_level_words.append('<EOS>')\n",
    "                    break\n",
    "                else:\n",
    "                    #translate decoder output into word and append\n",
    "                    decoded_level_words.append(output_set.index2word[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            decoded_output.append(decoded_level_words)\n",
    "            level_confidence = statistics.mean(top_values)\n",
    "            level_confidence_scores.append(level_confidence)\n",
    "            \n",
    "        #take average of decoded word values as confidence score\n",
    "        confidence_score = statistics.mean(level_confidence_scores)\n",
    "\n",
    "        return decoded_output, decoder_attentions[:de + 1], confidence_score\n",
    "    \n",
    "            \n",
    "def predict_on_new(input_set, output_set, encoder, decoder, model_type, df, max_length=100, verbose=False):\n",
    "    '''\n",
    "    For a list of legacy accounts, predict a target for each account. Returns a dataframe with the legacy accounts, predicted target accounts, and respective confidence scores.\n",
    "    '''\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "        source_data = {'Source Account Description': df['Source Account Description'].copy()}\n",
    "        \n",
    "    elif(model_type == 'long'):     \n",
    "        source_data = {'Source Level 1': df['Source Level 1'].copy(),\n",
    "                       'Source Level 2': df['Source Level 2'].copy(),\n",
    "                       'Source Level 3': df['Source Level 3'].copy(),\n",
    "                       'Source Account Description': df['Source Account Description'].copy()}\n",
    "        \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "\n",
    "    sources = pd.DataFrame(source_data)\n",
    "    \n",
    "    if(verbose): print('Generating predictions...')\n",
    "    predictions, confidence_scores = [], []\n",
    "    for a in tqdm(range(len(sources))):\n",
    "        account = sources.iloc[a, :].tolist()\n",
    "        #predict on input account\n",
    "        output, _, confidence_score = predict(input_set, output_set, encoder, decoder, account, model_type=model_type, max_length=max_length)\n",
    "        prediction = []\n",
    "        for pred_level in output:\n",
    "            pred_level = \" \".join(pred_level[:-1]) #drop EOS token\n",
    "            prediction.append(pred_level)\n",
    "            \n",
    "        predictions.append(prediction)\n",
    "        confidence_scores.append(confidence_score)\n",
    "\n",
    "    df_out = sources.copy()\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "            df_out['Prediction Account Description'] = [p[0] for p in predictions] #remove list brackets from each acct\n",
    "            df_out['Confidence Score'] = confidence_scores\n",
    "    else:     \n",
    "        df_out['Prediction Level 1'] = list(list(zip(*predictions))[0])\n",
    "        df_out['Prediction Level 2'] = list(list(zip(*predictions))[1])\n",
    "        df_out['Prediction Level 3'] = list(list(zip(*predictions))[2])\n",
    "        df_out['Prediction Account Description'] = list(list(zip(*predictions))[3])\n",
    "        df_out['Confidence Score'] = confidence_scores\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def evaluate_output(df_predictions, df_with_targets, model_type, verbose=True):\n",
    "    \n",
    "    #df_with_targets = df_with_targets.reset_index(drop=True)\n",
    "    if(model_type == 'short'):\n",
    "        target_data = {'Target Account Description': df_with_targets['Target Account Description']}\n",
    "        \n",
    "    elif(model_type == 'long'):\n",
    "        target_data = {'Target Level 1': df_with_targets['Target Level 1'],\n",
    "                       'Target Level 2': df_with_targets['Target Level 2'],\n",
    "                       'Target Level 3': df_with_targets['Target Level 3'],\n",
    "                       'Target Account Description': df_with_targets['Target Account Description']}\n",
    "    \n",
    "    else: raise ValueError(\"Invalid input for model type: acceptable arguments include ['short', 'long']\")\n",
    "    \n",
    "    df_targets = pd.DataFrame(target_data)\n",
    "    \n",
    "    predictions, targets = [], []\n",
    "    for i in range(len(df_predictions)):\n",
    "        if(model_type == 'short'):\n",
    "            predictions.append(df_predictions.loc[i, 'Prediction Account Description'])\n",
    "        else:\n",
    "            predictions.append(df_predictions.loc[i, 'Prediction Level 1':'Prediction Account Description'].tolist())\n",
    "        targets.append(df_targets.iloc[i, :].tolist())\n",
    "    \n",
    "    l1_scores, l2_scores, l3_scores, l4_scores, description_similarities = calc_scores(predictions, targets, model_type)\n",
    "    \n",
    "    if(model_type == 'short'):\n",
    "        score_data = {'Predicted Description Bleu Score': l4_scores,\n",
    "                      'Predicted Description Similarity':description_similarities}\n",
    "        if(verbose):\n",
    "             print(f'\\nDescription Bleu score average: {np.mean(l4_scores):.2f} \\\n",
    "                     \\nAverage predicted description (L4) similarity to target: {np.mean(description_similarities)*100:.2f}%')\n",
    "    else:\n",
    "        score_data = {'Predicted Level 1 Bleu Score': l1_scores, \n",
    "                      'Predicted Level 2 Bleu Score': l2_scores,\n",
    "                      'Predicted Level 3 Bleu Score': l3_scores,\n",
    "                      'Predicted Description Bleu Score': l4_scores,\n",
    "                      'Predicted Description Similarity':description_similarities}\n",
    "        if(verbose):\n",
    "             print(f'\\nL1 Bleu score average: {np.mean(l1_scores):.2f} \\\n",
    "                     \\nL2 Bleu score average: {np.mean(l2_scores):.2f} \\\n",
    "                     \\nL3 Bleu score average: {np.mean(l3_scores):.2f} \\\n",
    "                     \\nL4 Bleu score average: {np.mean(l4_scores):.2f} \\\n",
    "                     \\nAverage predicted description (L4) similarity to target: {np.mean(description_similarities)*100:.2f}%')\n",
    "    \n",
    "    df_scores = pd.DataFrame(score_data)\n",
    "    \n",
    "    df_out = pd.concat([df_predictions, df_targets, df_scores], axis=1)\n",
    "    \n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "model_path = cwd + '/'\n",
    "\n",
    "model_name = 'long_model_100000_0.734.hdf5'\n",
    "input_set, output_set, encoder, decoder, max_length, model_type = load_model(model_path, model_name, device, attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.concat([x_test, y_test], axis=1)\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "df_predictions = predict_on_unknown(input_set, output_set, encoder, decoder, model_type=model_type, df=data_test, max_length=max_length, verbose=True)\n",
    "if 'Target Account Description' in data_test.columns:\n",
    "    df_predictions = evaluate_output(df_predictions, data_test, model_type=model_type)\n",
    "    \n",
    "df_predictions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'short'\n",
    "\n",
    "source_data, target_data = format_df(df, model_type=model_type)\n",
    "x_train, x_test, y_train, y_test = preprocess_data_for_model(source_data, target_data, test_size=0.15)\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_set, output_set, pairs = create_vocabulary('legacy', 'new', x_train, y_train)\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_set.n_words, 256).to(device)\n",
    "decoder = AttnDecoder(256, output_set.n_words).to(device)\n",
    "\n",
    "save_path = os.getcwd()\n",
    "\n",
    "train_model(input_set, output_set, encoder, decoder, pairs, n_iters=100000, save_path=save_path, model_name='short_model',\n",
    "            model_type=model_type, hidden_size=256, dropout=0.1, attention=True, save_every=2000, learning_rate=0.01, benchmark_every=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.concat([x_test, y_test], axis=1)\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "df_predictions = predict_on_unknown(input_set, output_set, encoder, decoder, model_type=model_type, df=data_test, max_length=max_length, verbose=True)\n",
    "if 'Target Account Description' in data_test.columns:\n",
    "    df_predictions = evaluate_output(df_predictions, data_test, model_type=model_type)\n",
    "    \n",
    "df_predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "559d1aecbf8f97047d84863de8a860628de5e493631dd3e19f1951ec99c0a7ae"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
